{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26547793-2fce-4684-b38c-efdff474ba67",
   "metadata": {},
   "source": [
    "**Multi-Class Image Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b13697-7990-460b-b921-9e41a61d7658",
   "metadata": {},
   "source": [
    "* 1. Become one with the data\n",
    "  2. Preprocessing the data\n",
    "  3. Create a model\n",
    "  4. Fit the model\n",
    "  5. Evaluate the model\n",
    "  6. Adjust different hyperparameters and improve the model\n",
    "  7. Repeat until satisfied\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4719236-b409-417e-ad36-a86ed814f20a",
   "metadata": {},
   "source": [
    "Import and become one with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccac4a1-2dc0-4664-8fc4-a7d7a37bf596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-07 22:35:21--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.43.59, 142.251.43.91, 142.251.43.123, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.43.59|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 519183241 (495M) [application/zip]\n",
      "Saving to: ‘10_food_classes_all_data.zip.3’\n",
      "\n",
      "           10_food_  20%[===>                ] 103.37M  1.68MB/s    eta 3m 40s "
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"10_food_classes_all_data.zip\", \"r\")\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62940b0-547e-4e49-9579-f278f0642cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_all_data\"):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634dc773-c73f-4732-8e84-9e170075fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Setup the train and test directories\n",
    "\n",
    "train_dir = \"10_food_classes_all_data/train/\"\n",
    "test_dir = \"10_food_classes_all_data/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb4e01-26ed-45fc-90aa-8da683207f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "data_dir = pathlib.Path(train_dir)\n",
    "class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c02f21-951a-48cd-9137-46bfdf2c15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Cisualize our images\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "def view_random_image(target_dir, target_class):\n",
    "    target_folder = target_dir+target_class\n",
    "    \n",
    "    random_image = random.sample(os.listdir(target_folder), 1)\n",
    "    print(random_image)\n",
    "    img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
    "    plt.imshow(img)\n",
    "    plt.title(target_class)\n",
    "    plt.axis(\"off\");\n",
    "    print(f\"Image Shape: {img.shape}\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b73ff3-27a6-453a-9f1f-f4a1068332f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = view_random_image(target_dir=train_dir,\n",
    "                        target_class=random.choice(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674dc167-9065-44c3-b015-4dfbe953c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(train_dir,\n",
    "                                               batch_size=32, # number of images to process at a time \n",
    "                                               target_size=(224, 224), # convert all images to be 224 x 224\n",
    "                                               class_mode=\"categorical\")\n",
    "\n",
    "test_data = train_datagen.flow_from_directory(test_dir,\n",
    "                                               batch_size=32, # number of images to process at a time \n",
    "                                               target_size=(224, 224), # convert all images to be 224 x 224\n",
    "                                               class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6b755-8376-4b64-9ff1-dcc934d7586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#Make the createing of our modelf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model_8 = Sequential([\n",
    "    Conv2D(10, 3, input_shape=(224, 224, 3)),\n",
    "    Activation(activation=\"relu\"),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a5681-a2da-4922-975c-0208ade3fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model_8.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956337a3-f062-46a7-af99-20ab21e0db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_8 = model_8.fit(train_data,\n",
    "                  epochs=1,\n",
    "                  steps_per_epoch=len(train_data),\n",
    "                  validation_data=test_data,\n",
    "                  validation_steps=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9425b8ec-0ea1-4f2e-945e-8ab778aaa9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a63a4-dbe3-4808-bb6b-0c0bd4ef3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(history):\n",
    "  \"\"\"\n",
    "  Returns separate loss curves for training and validation metrics.\n",
    "  \"\"\" \n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  accuracy = history.history['accuracy']\n",
    "  val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "  epochs = range(len(history.history['loss']))\n",
    "\n",
    "  # Plot loss\n",
    "  plt.plot(epochs, loss, label='training_loss')\n",
    "  plt.plot(epochs, val_loss, label='val_loss')\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()\n",
    "\n",
    "  # Plot accuracy\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2537096-fb66-47d5-9ba9-c072a02df2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(h_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144cd69-eba7-467f-b5fd-59edc7c71487",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c57a6-a10a-4b83-b4ba-6cc05305fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_9 = Sequential([\n",
    "    Conv2D(10, 3, activation=\"relu\", input_shape=(224, 224, 3)),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation=\"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adcfd6-6508-4be0-8bec-27047f6dffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model_9.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_9.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f6f10-8055-4640-bf61-716cd32851ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_9 = model_9.fit(train_data,\n",
    "                  epochs=1,\n",
    "                  steps_per_epoch=len(train_data),\n",
    "                  validation_data=test_data,\n",
    "                  validation_steps=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea762b-5896-4b67-b6c5-d31466c38265",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(h_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b75ee-6fc4-48e9-bdc5-aafdc5726cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmented data generator instance\n",
    "train_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n",
    "                                             rotation_range=20, # note: this is an int not a float\n",
    "                                             width_shift_range=0.2,\n",
    "                                             height_shift_range=0.2,\n",
    "                                             zoom_range=0.2,\n",
    "                                             horizontal_flip=True)\n",
    "\n",
    "train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n",
    "                                                                  target_size=(224, 224),\n",
    "                                                                  batch_size=32,\n",
    "                                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550075e-ad6a-4fc0-9cbe-1701f0669806",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10 = Sequential([\n",
    "    Conv2D(32, 3, activation=\"relu\", input_shape=(224, 224, 3)),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(32, 3, activation=\"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(32, 3, activation=\"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(32, 3, activation=\"relu\"),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170dadc-c1bf-41f9-84e2-399a649322e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model_10.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09936a66-586f-4d1c-80e8-15be0d3a4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_10 = model_10.fit(train_data_augmented,\n",
    "                  epochs=15,\n",
    "                  steps_per_epoch=len(train_data_augmented),\n",
    "                  validation_data=test_data,\n",
    "                  validation_steps=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79849092-837b-4725-89dc-55f642150fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7d622-eb07-4161-a993-b07a5bcf42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(h_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc9842-28df-4141-a647-a784ee4a15a5",
   "metadata": {},
   "source": [
    "**Makig a predication with our trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c54d9-fc78-44c1-b162-082eb4d58043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4ed52-4e95-4ebb-bc98-30d9ef59a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to import and image and resize it to be used with our model\n",
    "def load_and_prep_image(filename, img_shape=224):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_image(img)\n",
    "    img = tf.image.resize(img, size=[img_shape, img_shape])\n",
    "    img = img/255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b1642-8e6f-43d9-adaf-732a8cc1f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust function to work with multi-class\n",
    "def pred_and_plot(model, filename, class_names):\n",
    "\n",
    "  # Import the target image and preprocess it\n",
    "  img = load_and_prep_image(filename)\n",
    "\n",
    "  # Make a prediction\n",
    "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
    "\n",
    "  # Get the predicted class\n",
    "  if len(pred[0]) > 1: # check for multi-class\n",
    "    pred_class = class_names[pred.argmax()] # if more than one output, take the max\n",
    "  else:\n",
    "    pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n",
    "\n",
    "  # Plot the image and predicted class\n",
    "  plt.imshow(img)\n",
    "  plt.title(f\"Prediction: {pred_class}\")\n",
    "  plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007dada-1f89-4b30-9b77-6319b1b40d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_and_plot(model = model_10,\n",
    "              filename=\"03-sushi.jpeg\",\n",
    "              class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb91d39-a1aa-4858-b626-de3184bde7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"saved_trained_model_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cc400-a412-485d-b82b-f6d33aef66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"saved_trained_model_10\")\n",
    "loaded_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658097f0-7261-4b4d-a0b7-cf2b18e53180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
